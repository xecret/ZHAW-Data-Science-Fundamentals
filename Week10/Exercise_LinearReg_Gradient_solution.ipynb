{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression & Gradient Descent\n",
    "\n",
    "In this notebook we will work with the diabetes dataset \n",
    "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
    "https://web.stanford.edu/~hastie/Papers/LARS/diabetes.data (raw data, not standardised)\n",
    "\n",
    "The data set contains 10 baseline variables (X1, X2, X3....X10) for 442 patients with diabetes: age, sex, body mass index, average blood pressure, and six blood serum measurements. The target variable (y) indicates the progression of the dieases one year after baseline.\n",
    "\n",
    "Luckily, this database is available in sklearn, so we need only to load it. \n",
    "Even more luckily, the sklearn database is already standardised (compare with the raw data above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a subset of the dataset, specifically only the variables: body mass index, average blood pressure and one blood serum measurement. Feel free to include more dimensions in your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X = diabetes_X[:,2:5] #select only the 3 relevant features we want to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, we split our dataset in training set and test set (z.B 80 data points are left as a test).\n",
    "We will fit(train) our model on the training dataset and then we will evaluate how they really perform on test data.\n",
    "The performance on the test data is fundamental to see how models generalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X_train = diabetes_X[:-80]\n",
    "diabetes_X_test = diabetes_X[-80:]\n",
    "\n",
    "diabetes_y_train = diabetes_y[:-80]\n",
    "diabetes_y_test = diabetes_y[-80:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook we want to first compare several methods to determine the best parameters for our model. \n",
    "Let's start with the LinearRegression() function from sklearn.\n",
    "The LinearRegression class is based on the scipy.linalg.lstsq() function (the name stands for \"least squares\"), this compute the optimal paramters'values using the least square exact method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters (theta_1, theta_2, theta_3, theta_0): \n",
      " [805.69232777 363.70203872  52.14450364] 152.53452637832743\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# The model's parameters\n",
    "print('Parameters (theta_1, theta_2, theta_3, theta_0): \\n', regr.coef_, regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to compute the same parameters using the Batch Gradient Descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iterations = 100000 #try with 1000, 10000, 100000...\n",
    "N = len(diabetes_X)\n",
    "\n",
    "theta_0 = np.random.randn(1) # random initialization\n",
    "theta_1 = np.random.randn(1)\n",
    "theta_2 = np.random.randn(1)\n",
    "theta_3 = np.random.randn(1)\n",
    "\n",
    "X1= diabetes_X_train[:,0]\n",
    "X2= diabetes_X_train[:,1]\n",
    "X3= diabetes_X_train[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code here the formulas for the batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "\n",
    "    \n",
    "    gradient_theta_3 = 1/N * np.sum((theta_0 + theta_1*X1 + theta_2*X2 + theta_3*X3  - diabetes_y_train)*X3)\n",
    "    gradient_theta_2 = 1/N * np.sum((theta_0 + theta_1*X1 + theta_2*X2 + theta_3*X3  - diabetes_y_train)*X2)\n",
    "    gradient_theta_1 = 1/N * np.sum((theta_0 + theta_1*X1 + theta_2*X2 + theta_3*X3  - diabetes_y_train)*X1)\n",
    "    gradient_theta_0 = 1/N * np.sum((theta_0 + theta_1*X1 + theta_2*X2 + theta_3*X3  - diabetes_y_train))\n",
    "    \n",
    "    \n",
    "    theta_3 = theta_3 - eta * gradient_theta_3\n",
    "    theta_2 = theta_2 - eta * gradient_theta_2                                \n",
    "    theta_1 = theta_1 - eta * gradient_theta_1\n",
    "    theta_0 = theta_0 - eta * gradient_theta_0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[805.68576868] [363.70743351] [52.14566178] [152.53451874]\n"
     ]
    }
   ],
   "source": [
    "#Let's see hwo the parameters look like \n",
    "print(theta_1,theta_2,theta_3, theta_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the values obtained with the function LinearRegression().\n",
    "Try to change the number of iterations and/or eta to get closer to the exact values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "We try now to see how the results change if we use the SGD approach. We use here the sklearn function SGDRegressor.\n",
    "Check out here the syntax\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "General Infos:\n",
    "https://scikit-learn.org/stable/modules/sgd.html#regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(max_iter=1000000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(max_iter=1000000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor(max_iter=1000000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(max_iter=1000000, eta0=0.01)\n",
    "sgd_reg.fit(diabetes_X_train, diabetes_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([627.88548258, 368.84550492, 123.33762085]), array([152.37568479]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.coef_, sgd_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare with the previous solutions? Try to adapt also here eta and number of iterations. \n",
    "How does it compare in terms of velocity with the Batch Gradient Descent?\n",
    "\n",
    "Try to run the previous 2 cells again. Do you get always the same result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you found the best parameters for your linear model, you can use them on the test data to predict values and evaluate how well your model perform, that is how much your predictions are off from the true y values of your test data.\n",
    "Let's start with the model obtained with LinearRegression() and then do the same with the linear regression obtained via SGD. \n",
    "How do the 2 compare on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_y_pred_EasyLinReg = regr.predict(diabetes_X_test)\n",
    "diabetes_y_pred_SGD = sgd_reg.predict(diabetes_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() Mean squared error: 3327.59\n",
      "SGD() Mean squared error: 3357.92\n"
     ]
    }
   ],
   "source": [
    "print('LinearRegression() Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred_EasyLinReg))\n",
    "print('SGD() Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred_SGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: Notebook adapted from A.Geron, Hands On ML with Scikit-Learn, Keras und Tensorflow, O'Reilly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSF_HS22",
   "language": "python",
   "name": "dsf_hs22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
